# Configuration for counting task with SGD baseline comparison
dataset:
  name: counting

model:
  arch: transformer
  model_count_times_batch_size: 800   # 20 models * 40 batch size = 800 (smaller for speed)
  init: regular

model.transformer:
  vocab_size: 110
  d_model: 32
  n_layers: 2  
  n_heads: 4
  d_ff: 64
  max_len: 32
  dropout: 0.1

optimizer:
  name: SGD
  lr: 0.01              # Reasonable learning rate for transformers
  momentum: 0.9         # Standard momentum
  epochs: 200           # More epochs than PatternSearch since SGD needs more iterations
  batch_size: 128         # Small batch size for better generalization
  es_acc: 0.95          # Same early stopping as PatternSearch
  
distributed:
  loss_thres: "0.1,0.3,0.5,1.0"      # Same loss thresholds for fair comparison
  num_samples: "300,400"               # Same sample sizes
  target_model_count_subrun: 5        # Same number of models per run

output:
  target_model_count: 5               # Same total target
  folder: "counting_transformer_sgd_results"